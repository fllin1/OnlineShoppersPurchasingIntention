{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **0.Librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, f1_score,\n",
    "                             make_scorer, roc_auc_score, roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, StratifiedKFold,\n",
    "                                     cross_val_score, cross_validate,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Scipy\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# XGBoost and LightGBM\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# UCI ML Repo\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# Custom scripts\n",
    "from scripts.data_preprocessing import fetch_data\n",
    "from scripts.data_visualisation import data_visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I. Problem Setting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Problem Definition**\n",
    "\n",
    "We will use the [Online Shoppers Purchasing Intention](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset) dataset to predict whether a user's session will result in a purchase or not. This is a **binary classification problem**.\n",
    "\n",
    "- **Goal:** Build a predictive model that can classify user sessions into two categories:\n",
    "    - **Revenue = True**: the session results in a purchase.\n",
    "    - **Revenue = False**: the session does not result in a purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Mathematical Formulation**\n",
    "\n",
    "Let:\n",
    "- A dataset $X=\\{x_1,x_2,...,x_n\\}$ where each $x_i$ is a feature vector representing a session.\n",
    "- A target variable $Y=\\{y_1, y_2, ..., y_n\\}$ where $y_i \\in \\{0, 1\\}$ (0 for \"no purchase\", 1 for \"purchase\").\n",
    "\n",
    "The goal is to find a function $f : X \\rightarrow Y$ such that for a new example $x$, $f(x)$ correctly predicts $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of feature vectors belonging to 12,330 sessions. \n",
    "\n",
    "The dataset was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user profile, or period.\n",
    "\n",
    "It cointains no missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset using fetch_ucirepo function from the UCIML repository\n",
    "online_shoppers_purchasing_intention_dataset = fetch_ucirepo(id=468) \n",
    "\n",
    "# Retrieve the variables/metadata associated with the dataset\n",
    "variables = online_shoppers_purchasing_intention_dataset.variables \n",
    "\n",
    "# Extract the feature matrix (X) and target vector (y) from the dataset\n",
    "X = online_shoppers_purchasing_intention_dataset.data.features \n",
    "y = online_shoppers_purchasing_intention_dataset.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data\n",
    "# variables.to_csv('../data/online_shoppers_purchasing_intention_dataset.csv', index=False)\n",
    "# X.to_csv('../data/features.csv', index=False)\n",
    "# y.to_csv('../data/targets.csv', index=False)\n",
    "\n",
    "# Load Data\n",
    "variables = pd.read_csv('../data/online_shoppers_purchasing_intention_dataset.csv')\n",
    "X = pd.read_csv('../data/features.csv')\n",
    "y = pd.read_csv('../data/targets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to the **general understanding of the variables** and will consist of the study of:\n",
    "- **Numerical variables:** Analysis of distributions (mean, median, standard deviation).\n",
    "- **Categorical variables:** Counting occurrences, encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Variables Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove empty columns\n",
    "variables = variables[['name', 'role', 'type', 'missing_values']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Target: Revenue (y)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y['Revenue'].sum()/len(y))\n",
    "\n",
    "# On affiche la distribution de la variable cible\n",
    "sns.countplot(x='Revenue', data=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that only **14,5%** of users bought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Features (X)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is indeed no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Initial Visualisations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Visualisation of numerical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numeric features from the dataset\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Determine the number of rows and columns for subplots\n",
    "n = len(numeric_features)\n",
    "cols = 4  # Number of columns for the grid layout\n",
    "rows = n // cols + (n % cols > 0)  # Number of rows, accounting for remaining features\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 12))  # Define the figure size\n",
    "axes = axes.flatten()  # Flatten the axes array to easily iterate through\n",
    "\n",
    "# Plot each numeric feature's distribution\n",
    "for i, feature in enumerate(numeric_features):\n",
    "    sns.histplot(X[feature], kde=True, ax=axes[i])  # Plot histogram and KDE for each feature\n",
    "    axes[i].set_title(f'Distribution of {feature}')  # Set title for each subplot\n",
    "\n",
    "# Remove unused subplots if there are any extra axes\n",
    "for i in range(n, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust the layout for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()  # Display the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the distributions show a high concentration of data **close to zero**, particularly for interactions with specific types of pages (administrative, informational, and product-related pages). We can assume that only a small fraction of users engage with these types of pages.\n",
    "\n",
    "- Analyzing the exit and bounce rates could help **identify engagement issues**, especially if certain types of pages (such as product or informational pages) have higher exit rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the categorical features (object or boolean type) from the dataset\n",
    "categorical_features = X.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "\n",
    "# Determine the number of categorical features\n",
    "n = len(categorical_features)\n",
    "cols = 3  # Number of columns for the grid layout\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(1, cols, figsize=(16, 4))  # Define the figure size for a single row\n",
    "axes = axes.flatten()  # Flatten the axes array to iterate through\n",
    "\n",
    "# Plot the distribution of each categorical feature\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    sns.histplot(X[feature], kde=True, ax=axes[i])  # Plot histogram for each feature\n",
    "    axes[i].set_title(f'Distribution of {feature}')  # Set the title for each subplot\n",
    "\n",
    "# Remove unused subplots if there are any extra axes\n",
    "for i in range(n, len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust the layout for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()  # Display the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several observations can be made:\n",
    "- There is a **seasonality pattern**, with peaks in the spring and fall.\n",
    "\n",
    "- A stronger presence of **returning visitors** indicates that the site has a good retention rate.\n",
    "\n",
    "- Purchases appear to be **evenly distributed** throughout the week, with a consistent proportion of about 5/7 on weekdays and 2/7 on weekends. This might suggest that the site is used equally for both professional and recreational purposes.\n",
    "\n",
    "There are no missing values in either the **features** or the **target** dataset, so we can directly proceed with working on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **b. Correlation Matrix :**\n",
    "To identify linked data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specified size\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Compute the correlation matrix for the numeric features\n",
    "correlation = X[numeric_features].corr()\n",
    "\n",
    "# Create a heatmap to visualize the correlation matrix\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')  # 'coolwarm' colormap for better visualization\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strong Correlations:**\n",
    "\n",
    "- **ProductRelated** and **ProductRelated_Duration** (0.86): It makes sense that the number of product pages visited is strongly correlated with the time spent on those pages. The more product pages users visit, the more time they spend on those pages.\n",
    "\n",
    "- **BounceRates** and **ExitRates** (0.91): These two variables are also highly correlated. This indicates that sessions with a high bounce rate tend to end quickly (high probability of leaving the site after viewing few pages).\n",
    "\n",
    "**Moderate Correlations:**\n",
    "\n",
    "- **Informational** and **Informational_Duration** (0.62): Similarly, there is a fairly high correlation between the number of informational pages visited and the time spent on those pages, which is also intuitive.\n",
    "\n",
    "- **Administrative** and **Administrative_Duration** (0.6): The relationship between the number of administrative pages visited and the time spent on those pages is moderate. The more users interact with administrative pages, the more time they spend on them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `sns.violinplot` to observe the distributions of **feature** against **target**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the data based on the specified percentiles for a given feature\n",
    "def filter_percentiles(df, feature, lower_percentile=0.10, upper_percentile=0.90):\n",
    "    lower_bound = df[feature].quantile(lower_percentile)\n",
    "    upper_bound = df[feature].quantile(upper_percentile)\n",
    "    # Return the filtered dataframe based on the lower and upper percentiles\n",
    "    return df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
    "\n",
    "data = pd.concat([X, y], axis=1)\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(4, 4, i) \n",
    "    # Filter the data based on the 10th and 90th percentiles of the feature\n",
    "    data_filtered = filter_percentiles(data, feature)\n",
    "    sns.violinplot(x='Revenue', y=feature, data=data_filtered, split=True) \n",
    "    plt.title(f'{feature} vs Revenue (80% values)')  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Product Pages**: Variables related to product pages (`ProductRelated` and `ProductRelated_Duration`) are the best indicators for distinguishing users who generate revenue from those who don't. The more a user interacts with product pages, the more likely they are to generate revenue.\n",
    "\n",
    "- **Bounce Rates** and **Exit Rates**: Users who generate revenue tend to have lower bounce and exit rates, indicating deeper engagement with the site.\n",
    "\n",
    "- **PageValues**: This is one of the most discriminative variables, with users generating revenue having much higher page values.\n",
    "\n",
    "- Other factors do not seem to have a significant impact on revenue generation, at least in this visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III. Modelisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Categorial Variables Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting `bool` into `int` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Weekend'] = data['Weekend'].astype(int)\n",
    "data['Revenue'] = data['Revenue'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding for multi-categorial data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['Month', 'VisitorType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Numeric Variables Standardisation**\n",
    "Unifying the scale of the Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **c. Class Unbalance**\n",
    "SMOTE to over-sample the minoritary class :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Revenue', axis=1)\n",
    "y = data['Revenue']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **d. Division into training and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, \n",
    "    y_resampled, \n",
    "    test_size=0.3, # 30% of the data is used for testing\n",
    "    random_state=42,\n",
    "    stratify=y_resampled # Stratification \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Classification Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following classic methods and then compare their performances:\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- Neural Network with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Models Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the variables to a tensor\n",
    "X_train_tensor = torch.tensor(X_train.values.astype(float), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values.astype(float), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    # Define the layers and activation functions in the constructor\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64) # Input layer with 64 neurons\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 2) # Output layer with 2 classes\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.layer1(x)) # ReLU activation\n",
    "        out = self.relu(self.layer2(out)) # ReLU activation\n",
    "        out = self.output(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model, define the loss function and optimizer\n",
    "model = NeuralNet(input_size=X_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 20 # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate the loss\n",
    "        \n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update the weights\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0) \n",
    "        correct += (predicted == labels).sum().item() \n",
    "        all_preds.extend(predicted.numpy()) \n",
    "    \n",
    "    print(f'Accuracy du réseau sur les données de test : {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision alone loses reliability in a scenario where the classes are imbalanced. Therefore, we will use 3 evaluation methods that are particularly relevant in this case, and are imported from `sklearn.metrics`:\n",
    "\n",
    "- `classification_report`:\n",
    "  - the **precision** of the model (proportion of positive predictions that are correct).\n",
    "  - the **recall** (proportion of actual positives that are correctly predicted).\n",
    "  - the **f1-score** (harmonic mean of precision and recall).\n",
    "\n",
    "- `confusion_matrix`: displays the **True Positives**, **True Negatives**, **False Positives**, and **False Negatives** within a 2x2 matrix.\n",
    "\n",
    "- `roc_auc_score`: **ROC** (Receiver Operating Characteristic) is the plot of the **True Positive Rate** against the **False Positive Rate** for each possible classification threshold between 0 and 1 (in practice, selected intervals). **AUC** is the area under the ROC curve; the closer it is to 1, the better the model performs. If it is close to 0.5, it is equivalent to random classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store predictions from different models\n",
    "predictions = {\n",
    "    'Logistic Regression': y_pred_lr,  # Predictions from Logistic Regression\n",
    "    'Decision Tree': y_pred_dt,        # Predictions from Decision Tree\n",
    "    'Random Forest': y_pred_rf,        # Predictions from Random Forest\n",
    "    'XGBoost': y_pred_xgb,             # Predictions from XGBoost\n",
    "    'NN (PyTorch)': all_preds  # Predictions from the PyTorch Neural Network\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store results for each model\n",
    "results = []\n",
    "\n",
    "# Loop through each model and its predictions\n",
    "for model_name, y_pred in predictions.items():\n",
    "    # Get the classification report as a dictionary\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Extract precision, recall, and f1-score for both classes (0 and 1)\n",
    "    precision_0 = report['0']['precision']\n",
    "    precision_1 = report['1']['precision']\n",
    "    recall_0 = report['0']['recall']\n",
    "    recall_1 = report['1']['recall']\n",
    "    f1_0 = report['0']['f1-score']\n",
    "    f1_1 = report['1']['f1-score']\n",
    "\n",
    "    # Append the results for each model to the results list\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Precision False': precision_0,\n",
    "        'Precision True': precision_1,\n",
    "        'Recall False': recall_0,\n",
    "        'Recall True': recall_1,\n",
    "        'F1-Score False': f1_0,\n",
    "        'F1-Score True': f1_1\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results  # Display the results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "metrics = ['Precision False', 'Recall False', 'F1-Score False', 'Precision True', 'Recall True', 'F1-Score True']\n",
    "titles = ['Precision False', 'Recall False', 'F1-Score False', 'Precision True', 'Recall True', 'F1-Score True']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    row, col = divmod(i, 3)  \n",
    "    sns.barplot(x='Model', y=metric, data=df_results, ax=axes[row, col], palette='viridis', hue='Model')\n",
    "    axes[row, col].tick_params(axis='x', rotation=30)\n",
    "    axes[row, col].set_title(titles[i])\n",
    "    axes[row, col].set_ylabel('Score')\n",
    "    axes[row, col].set_xlabel('Model')\n",
    "    axes[row, col].set_ylim((0.8, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Random Forest** seems to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a 2x3 grid of subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Loop through each model and its predictions to plot the confusion matrices\n",
    "for i, (model_name, y_pred) in enumerate(predictions.items()):\n",
    "    ax = axes[i // 3, i % 3]  # Determine the correct subplot location\n",
    "    cm = confusion_matrix(y_test, y_pred)  # Compute the confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='plasma', ax=ax)  # Plot the confusion matrix heatmap\n",
    "    ax.set_title(f'Confusion Matrix - {model_name}')  # Set title\n",
    "    ax.set_xlabel('Predictions')  # Set x-axis label\n",
    "    ax.set_ylabel('True Labels')  # Set y-axis label\n",
    "\n",
    "# Remove the unused last subplot (bottom right)\n",
    "axes[1][2].axis('off')\n",
    "\n",
    "# Adjust layout to prevent overlap and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Random Forest** and **XGBoost** slightly better in limiting False Positives and False Negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ROC-AUC Score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the **ROC curve** at different classification thresholds, we calculate the probabilities of belonging to each class for each observation. In practice, we are only interested in the probability $P(class=1)$. To obtain these probabilities, we can use the `predict_proba` method from `sklearn`. However, our **Neural Network (NN)** does not have this method, so we will compute the **logits** and then convert them to probabilities using the **sigmoid** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test set to a PyTorch tensor\n",
    "X_test_tensor = torch.FloatTensor(X_test.values.astype(float))\n",
    "\n",
    "# Perform inference with the neural network and calculate probabilities using the sigmoid function\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor)  # Get the raw logits from the model\n",
    "    probs_pytorch = F.sigmoid(logits).numpy()[:, 1]  # Apply sigmoid and convert to probabilities for class 1\n",
    "\n",
    "# Dictionary to store the predicted probabilities for each model\n",
    "probabilities = {\n",
    "    'Logistic Regression': lr.predict_proba(X_test)[:, 1],  # Probabilities for Logistic Regression\n",
    "    'Decision Tree': dt.predict_proba(X_test)[:, 1],        # Probabilities for Decision Tree\n",
    "    'Random Forest': rf.predict_proba(X_test)[:, 1],        # Probabilities for Random Forest\n",
    "    'XGBoost': xgb_model.predict_proba(X_test)[:, 1],       # Probabilities for XGBoost\n",
    "    'Neural Network (PyTorch)': probs_pytorch               # Probabilities from the PyTorch Neural Network\n",
    "}\n",
    "\n",
    "# Create a figure for plotting the ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Loop through each model and its probabilities to plot the ROC curve\n",
    "for model_name, probs in probabilities.items():\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs)  # Calculate FPR and TPR\n",
    "    roc_auc = roc_auc_score(y_test, probs)  # Calculate the AUC score\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')  # Plot the ROC curve\n",
    "\n",
    "# Plot the random guessing line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curves for Multiple Models')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that the **Random Forest**, **XGBoost**, and **Neural Networks** models outperform the others in terms of their ability to distinguish between classes, with AUC values close to 1.\n",
    "\n",
    "Moving forward, we will focus on the **Random Forest** model, as it provides the best overall results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Hyperparameters Optimisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **a. Grid Search for Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Grid Search` is a technique used to find the optimal hyperparameters of a machine learning model by testing all possible combinations of the specified hyperparameters and determining which combination provides the best performance based on a specific metric (such as precision, recall, or F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to test in Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'class_weight': ['balanced', 'balanced_subsample']  # Handle class imbalance\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with Random Forest and the parameter grid\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1', n_jobs=-1)  # F1-score for evaluation\n",
    "grid_search.fit(X_train, y_train)  # Fit the model using Grid Search\n",
    "\n",
    "# Get the best Random Forest model after Grid Search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict the labels using the best Random Forest model\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Print the best hyperparameters found by Grid Search\n",
    "print(\"Best parameters:\")\n",
    "pd.DataFrame(grid_search.best_params_, index=[\"Parameters\"])  # Display the best parameters in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store predictions for Random Forest models\n",
    "predictions_rf = {\n",
    "    'Random Forest': y_pred_rf,  # Original Random Forest\n",
    "    'Random Forest (Grid Search)': y_pred_best_rf  # Optimized Random Forest (after Grid Search)\n",
    "}\n",
    "\n",
    "# List to store results for each Random Forest model\n",
    "results_rf = []\n",
    "\n",
    "# Loop through each model's predictions and calculate the classification report metrics\n",
    "for model_name, y_pred in predictions_rf.items():\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)  # Get the classification report as a dictionary\n",
    "    precision_0 = report['0']['precision']  # Precision for class 0\n",
    "    precision_1 = report['1']['precision']  # Precision for class 1\n",
    "    recall_0 = report['0']['recall']        # Recall for class 0\n",
    "    recall_1 = report['1']['recall']        # Recall for class 1\n",
    "    f1_0 = report['0']['f1-score']          # F1-score for class 0\n",
    "    f1_1 = report['1']['f1-score']          # F1-score for class 1\n",
    "\n",
    "    # Append results for each model\n",
    "    results_rf.append({\n",
    "        'Model': model_name,\n",
    "        'Precision False': precision_0,\n",
    "        'Precision True': precision_1,\n",
    "        'Recall False': recall_0,\n",
    "        'Recall True': recall_1,\n",
    "        'F1-Score False': f1_0,\n",
    "        'F1-Score True': f1_1\n",
    "    })\n",
    "\n",
    "# Convert the results into a DataFrame for better display\n",
    "df_results_rf = pd.DataFrame(results_rf)\n",
    "df_results_rf  # Display the DataFrame with the results for both models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances are extremely close to our initial model. Only the **F1-Score** shows a noticeable improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **b. Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conduct a **robustness test** using the `Stratified K-Fold Cross-Validation` method.\n",
    "\n",
    "What it involves:\n",
    "- **Stratification**: Ensures that each fold has approximately the same proportion of each class as the original dataset.\n",
    "\n",
    "- **K-Fold**: The dataset is divided into K equally-sized subsets.\n",
    "\n",
    "- **Cross Validation**: This involves training and testing the model on the different subsets.\n",
    "\n",
    "The objectives are as follows:\n",
    "- **More reliable estimation**: Reduces the variance associated with a single train/test split.\n",
    "\n",
    "- **Efficient use of data**: All observations are used for both training and testing.\n",
    "\n",
    "- **Detection of overfitting**: Allows us to verify if the model generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds for Stratified K-Fold Cross-Validation\n",
    "k = 5\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the scoring metrics for evaluation\n",
    "scoring = {\n",
    "    'precision': 'precision',  # Precision metric\n",
    "    'recall': 'recall',        # Recall metric\n",
    "    'f1': 'f1',                # F1-score metric\n",
    "    'roc_auc': 'roc_auc'       # ROC-AUC metric\n",
    "}\n",
    "\n",
    "# Create a pipeline with SMOTE for class imbalance, a scaler (if needed), and the best Random Forest model\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),  # Handle class imbalance with SMOTE\n",
    "    ('scaler', StandardScaler()),       # Apply standard scaling (if necessary)\n",
    "    ('classifier', best_rf)             # Use the optimized Random Forest model\n",
    "])\n",
    "\n",
    "# Perform cross-validation using the pipeline, Stratified K-Fold, and the defined scoring metrics\n",
    "cv_results = cross_validate(\n",
    "    estimator=pipeline,\n",
    "    X=X,  # Feature set\n",
    "    y=y,  # Target variable\n",
    "    cv=skf,  # Stratified K-Fold cross-validation\n",
    "    scoring=scoring,  # Evaluation metrics\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Print the mean and standard deviation for each metric\n",
    "for metric in scoring.keys():\n",
    "    scores = cv_results[f'test_{metric}']\n",
    "    print(f\"{metric} : Mean = {scores.mean():.4f}, Standard Deviation = {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-validation results show that the model is not very stable, and its performance is not very robust, especially when looking at `precision`, `recall`, and `f1` across different subsamples. However, the slightly lower average performance compared to the results on the full dataset can be explained by the fact that the model is being tested on portions of the data that are too small during each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV. Results Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Variables Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the best Random Forest model\n",
    "importances = best_rf.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Create a barplot of the top 10 most important features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "sns.barplot(x=X_train.columns[indices][:10], y=importances[indices][:10])  # Plot the top 10 features\n",
    "\n",
    "# Rotate the x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PageValues` is by far the most explanatory variable. This makes sense, as **PageValues** is a value based on the probability that a specific page leads to a conversion, referring to the site's historical data.\n",
    "\n",
    "We also notice that **categorical variables** are not among the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Error Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'lle keep only the most important **10 features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = X_train.columns[indices][:10]\n",
    "\n",
    "# Adding the true and predicted labels to the DataFrame\n",
    "X_test_df = X_test.copy()[top_features]\n",
    "X_test_df['True_Label'] = y_test.values\n",
    "X_test_df['Predicted_Label'] = y_pred_best_rf\n",
    "\n",
    "# Misclassified instances\n",
    "misclassified = X_test_df[X_test_df['True_Label'] != X_test_df['Predicted_Label']]\n",
    "print(f\"Nombre d'instances mal classées : {len(misclassified)} sur {len(X_test_df)} total\")\n",
    "\n",
    "# Correctly classified instances\n",
    "correctly_classified = X_test_df[X_test_df['True_Label'] == X_test_df['Predicted_Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation for the top features in misclassified instances\n",
    "misclassified_stats = misclassified[top_features].describe().T[['mean', 'std']]\n",
    "\n",
    "# Mean and standard deviation for the top features in correctly classified instances\n",
    "correctly_classified_stats = correctly_classified[top_features].describe().T[['mean', 'std']]\n",
    "\n",
    "# Stats for comparison\n",
    "comparison_stats = misclassified_stats.join(correctly_classified_stats, lsuffix='_misclassified', rsuffix='_correctly_classified')\n",
    "comparison_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will observe the **density** distributions of each feature based on whether they were correctly classified or not.\n",
    "\n",
    "This may potentially help us identify characteristics in the data that led to misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_test_df.select_dtypes(include=[np.number]).columns.tolist() # Get the numeric features\n",
    "numeric_features.remove('True_Label') # Remove the 'True_Label' column\n",
    "numeric_features.remove('Predicted_Label')  # Remove the 'Predicted_Label' column\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    \n",
    "    # Compute the min and max values for the x-axis with a margin of 1.96 * std\n",
    "    # (95% confidence interval for a normal distribution)\n",
    "    min_val = min(comparison_stats.loc[feature, 'mean_misclassified'] - 1.96 * comparison_stats.loc[feature, 'std_misclassified'],\n",
    "                  comparison_stats.loc[feature, 'mean_correctly_classified'] - 1.96 * comparison_stats.loc[feature, 'std_correctly_classified'])\n",
    "    max_val = max(comparison_stats.loc[feature, 'mean_misclassified'] + 1.96 * comparison_stats.loc[feature, 'std_misclassified'],\n",
    "                  comparison_stats.loc[feature, 'mean_correctly_classified'] + 1.96 * comparison_stats.loc[feature, 'std_correctly_classified'])\n",
    "    \n",
    "    # Create a subplot for each feature\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.kdeplot(data=correctly_classified, x=feature, label='Bien classés', fill=True)\n",
    "    sns.kdeplot(data=misclassified, x=feature, label='Mal classés', fill=True)\n",
    "    \n",
    "    # Set the x-axis limits and title for each subplot\n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.title(f'Distribution de {feature}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look only at the four most important variables, we can see that the distributions for `ProductRelated_Duration` and `Administrative` are quite clearly distinct. However, those for `PageValues` and `ExitRates` are not as distinct as we might expect.\n",
    "\n",
    "We can conduct statistical tests to study these distributions in more detail. If the `p-values` are close to 0, this means that the differences between the distributions are **significant**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_data = []\n",
    "\n",
    "for feature in numeric_features:\n",
    "    # Perform the Kolmogorov-Smirnov test\n",
    "    stat, p_value = ks_2samp(correctly_classified[feature], misclassified[feature]) \n",
    "    ks_data.append([f'{stat:.3f}', f'{p_value:.3f}'])\n",
    "pd.DataFrame(ks_data, columns=['Statistique KS', 'p-value'], index=numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 8 most important **features** show significantly different distributions.\n",
    "\n",
    "This highlights some behaviors of our model. For example, for the misclassified cases, the distributions are more spread out with very low and very high values. Additionally, there seems to be a trend where the means of the distributions for the misclassified cases shift more \"to the right.\"\n",
    "\n",
    "Indeed, as we have already observed in our DataFrame of means and standard deviations, the distributions for the misclassified cases tend to have higher `mean` and `std` values.\n",
    "\n",
    "We could consider **segmenting** the data based on `PageValues` and training specific models while adding **transformations** to better capture the extremes. The exploration of methods to improve our model's performance will be explored in a second part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used various machine learning methods to predict users' online purchasing intentions by analyzing the \"Online Shoppers Purchasing Intention\" dataset. By applying different preprocessing techniques, such as encoding categorical variables, normalizing numerical variables, and handling class imbalance via oversampling with `SMOTE`, we trained several classification models. Among these models, **Random Forest** and **XGBoost** demonstrated the best performance, with high `F1` scores, indicating a good ability to distinguish between sessions that resulted in a purchase and those that did not.\n",
    "\n",
    "The analysis of variables and errors revealed that some features, such as \"`PageValues`\" and \"`ProductRelated_Duration`\", were particularly crucial in making predictions. However, the model showed limitations in correctly classifying certain sessions, especially those exhibiting atypical behaviors or falling at the extremes of the variable distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI. To Go Farther**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several avenues for improvement can be explored to enhance the robustness and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing Optimization\n",
    "\n",
    "- **Advanced Feature Engineering**: Creating new variables by combining existing information could help capture non-linear relationships or complex interactions between variables.\n",
    "\n",
    "- **Variable Transformation**: Applying logarithmic transformations or robust normalization could improve the handling of extreme values and reduce the impact of outliers on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Integration of Clustering Methods\n",
    "\n",
    "- **Session Segmentation**: Using clustering algorithms such as `K-Means` or `DBSCAN`, we could identify homogeneous groups of sessions based on user behavior.\n",
    "\n",
    "- **Anomaly Detection**: Clustering can also help identify atypical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Class Imbalance Management\n",
    "\n",
    "- **Ensemble Techniques**: Using methods such as `EasyEnsemble` or `BalancedBaggingClassifier` could improve performance on the minority class by combining the predictions of several models trained on balanced subsamples.\n",
    "\n",
    "- **Class Weighting**: Adjusting the weights associated with classification errors to penalize false negatives more heavily could improve recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
